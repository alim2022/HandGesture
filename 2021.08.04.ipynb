{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aeddbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b3c1e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb8bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple method\n",
    "def simple_classification(hand_landmarks):\n",
    "    finger_up = [False, False, False, False, False]\n",
    "    \n",
    "    if(hand_landmarks.landmark[4].y < hand_landmarks.landmark[3].y\n",
    "       and hand_landmarks.landmark[3].y < hand_landmarks.landmark[2].y\n",
    "       and hand_landmarks.landmark[2].y < hand_landmarks.landmark[1].y\n",
    "       and hand_landmarks.landmark[1].y < hand_landmarks.landmark[0].y\n",
    "       and hand_landmarks.landmark[3].y < hand_landmarks.landmark[6].y \n",
    "      ):\n",
    "        finger_up[0] = True\n",
    "    if(hand_landmarks.landmark[8].y < hand_landmarks.landmark[7].y\n",
    "       and hand_landmarks.landmark[7].y < hand_landmarks.landmark[6].y\n",
    "       and hand_landmarks.landmark[6].y < hand_landmarks.landmark[5].y\n",
    "       and hand_landmarks.landmark[5].y < hand_landmarks.landmark[0].y\n",
    "      ):\n",
    "        finger_up[1] = True\n",
    "    if(hand_landmarks.landmark[12].y < hand_landmarks.landmark[11].y\n",
    "       and hand_landmarks.landmark[11].y < hand_landmarks.landmark[10].y\n",
    "       and hand_landmarks.landmark[10].y < hand_landmarks.landmark[9].y\n",
    "       and hand_landmarks.landmark[9].y < hand_landmarks.landmark[0].y\\\n",
    "      ):\n",
    "        finger_up[2] = True\n",
    "    if(hand_landmarks.landmark[16].y < hand_landmarks.landmark[15].y\n",
    "       and hand_landmarks.landmark[15].y < hand_landmarks.landmark[14].y\n",
    "       and hand_landmarks.landmark[14].y < hand_landmarks.landmark[13].y\n",
    "       and hand_landmarks.landmark[13].y < hand_landmarks.landmark[0].y\n",
    "      ):\n",
    "        finger_up[3] = True\n",
    "    if(hand_landmarks.landmark[20].y < hand_landmarks.landmark[19].y \n",
    "       and hand_landmarks.landmark[19].y < hand_landmarks.landmark[18].y \n",
    "       and hand_landmarks.landmark[18].y < hand_landmarks.landmark[17].y\n",
    "       and hand_landmarks.landmark[17].y < hand_landmarks.landmark[0].y\n",
    "      ):\n",
    "        finger_up[4] = True\n",
    "    \n",
    "    #print(finger_up)\n",
    "    \n",
    "    if(finger_up[1] and not finger_up[2] and not finger_up[3] and not finger_up[4]):\n",
    "        return 0\n",
    "    elif(finger_up[1] and finger_up[2] and not finger_up[3] and not finger_up[4]):\n",
    "        return 1\n",
    "    elif(finger_up[1] and finger_up[2] and finger_up[3] and not finger_up[4]):\n",
    "        return 2\n",
    "    elif(finger_up[0] and not finger_up[1] and not finger_up[2] and not finger_up[3] and not finger_up[4]):\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85966a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"main_data.pkl\" , \"rb\") as f:\n",
    "    training_dataset = pickle.load(f)\n",
    "with open(\"test_data.pkl\" , \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55523369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 100\n"
     ]
    }
   ],
   "source": [
    "print(len(training_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76bf4ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 91.0 %\n"
     ]
    }
   ],
   "source": [
    "correct_cnt = 0\n",
    "total_cnt = 0\n",
    "for data in test_dataset:\n",
    "    hand_landmarks, true_class = data\n",
    "    predict_class = simple_classification(hand_landmarks)\n",
    "    if (predict_class == true_class):\n",
    "        correct_cnt +=1\n",
    "    total_cnt+=1\n",
    "print(f\"accuracy : {correct_cnt * 100/total_cnt} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12ad2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.  0.  0.  0.  0.]\n",
      " [ 0. 20.  0.  0.  0.]\n",
      " [ 0.  0. 20.  0.  0.]\n",
      " [ 0.  0.  0. 11.  9.]\n",
      " [ 0.  0.  0.  0. 20.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix = np.zeros((5,5))\n",
    "for data in test_dataset:\n",
    "    hand_landmarks, true_class = data\n",
    "    predict_class = simple_classification(hand_landmarks)\n",
    "    confusion_matrix[true_class, predict_class] +=1\n",
    "print(confusion_matrix)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(confusion_matrix)\n",
    "ax.set_title(\"confusion matrix\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"simple_confusion.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d24f440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complex method\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "318c904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in training_dataset:\n",
    "    hand_landmarks, true_class = data\n",
    "\n",
    "    adjacency_list = [\n",
    "        [0,1,2],[1,2,3],[2,3,4],[0,5,6],[5,6,7],[6,7,8],\n",
    "        [0,9,10],[9,10,11],[10,11,12],[0,13,14],[13,14,15],\n",
    "        [14,15,16],[0,17,18],[17,18,19],[18,19,20]\n",
    "    ]\n",
    "\n",
    "    angle_list=[]\n",
    "\n",
    "    for adjacency in adjacency_list :\n",
    "        idx0, idx1, idx2 = adjacency\n",
    "        point0 = np.array([hand_landmarks.landmark[idx0].x, hand_landmarks.landmark[idx0].y, hand_landmarks.landmark[idx0].z])\n",
    "        point1 = np.array([hand_landmarks.landmark[idx1].x, hand_landmarks.landmark[idx1].y, hand_landmarks.landmark[idx1].z])\n",
    "        point2 = np.array([hand_landmarks.landmark[idx2].x, hand_landmarks.landmark[idx2].y, hand_landmarks.landmark[idx2].z])\n",
    "\n",
    "        vector0 = point0 - point1\n",
    "        vector1 = point2 - point1\n",
    "        inner_product_result = np.dot(vector0,vector1)\n",
    "        cos_theta = inner_product_result/(np.linalg.norm(vector0) * np.linalg.norm(vector1))\n",
    "        theta = np.arccos(cos_theta)\n",
    "        angle_list.append(theta)\n",
    "    \n",
    "    new_dataset.append([angle_list, true_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b68c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_training_dataset.pkl\" , \"wb\") as f:\n",
    "    pickle.dump(new_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1221e72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_inputs = []\n",
    "training_labels = []\n",
    "for data in new_dataset:\n",
    "    angle_list, true_class = data\n",
    "    training_inputs.append(angle_list)\n",
    "    training_labels.append(true_class)\n",
    "classifier.fit(training_inputs, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66156f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_dataset = []\n",
    "for data in test_dataset:\n",
    "    hand_landmarks, true_class = data\n",
    "\n",
    "    adjacency_list = [\n",
    "        [0,1,2],[1,2,3],[2,3,4],[0,5,6],[5,6,7],[6,7,8],\n",
    "        [0,9,10],[9,10,11],[10,11,12],[0,13,14],[13,14,15],\n",
    "        [14,15,16],[0,17,18],[17,18,19],[18,19,20]\n",
    "    ]\n",
    "\n",
    "    angle_list=[]\n",
    "\n",
    "    for adjacency in adjacency_list :\n",
    "        idx0, idx1, idx2 = adjacency\n",
    "        point0 = np.array([hand_landmarks.landmark[idx0].x, hand_landmarks.landmark[idx0].y, hand_landmarks.landmark[idx0].z])\n",
    "        point1 = np.array([hand_landmarks.landmark[idx1].x, hand_landmarks.landmark[idx1].y, hand_landmarks.landmark[idx1].z])\n",
    "        point2 = np.array([hand_landmarks.landmark[idx2].x, hand_landmarks.landmark[idx2].y, hand_landmarks.landmark[idx2].z])\n",
    "\n",
    "        vector0 = point0 - point1\n",
    "        vector1 = point2 - point1\n",
    "        inner_product_result = np.dot(vector0,vector1)\n",
    "        cos_theta = inner_product_result/(np.linalg.norm(vector0) * np.linalg.norm(vector1))\n",
    "        theta = np.arccos(cos_theta)\n",
    "        angle_list.append(theta)\n",
    "    \n",
    "    new_test_dataset.append([angle_list, true_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f98f52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs = []\n",
    "test_labels = []\n",
    "for data in new_test_dataset:\n",
    "    joint_angle, true_class = data\n",
    "    test_inputs.append(joint_angle)\n",
    "    test_labels.append(true_class)\n",
    "classifier.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37fb5c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 95.0 %\n"
     ]
    }
   ],
   "source": [
    "correct_cnt = 0\n",
    "total_cnt = 0\n",
    "for data in new_test_dataset:\n",
    "    angle_list, true_class = data\n",
    "    predict_class = classifier.predict([angle_list])[0]\n",
    "    if (predict_class == true_class):\n",
    "        correct_cnt +=1\n",
    "    total_cnt+=1\n",
    "print(f\"accuracy : {correct_cnt * 100/total_cnt} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5fecc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.  0.  0.  0.  0.]\n",
      " [ 0. 20.  0.  0.  0.]\n",
      " [ 0.  0. 20.  0.  0.]\n",
      " [ 0.  0.  0. 20.  0.]\n",
      " [ 0.  0.  0.  5. 15.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix = np.zeros((5,5))\n",
    "for data in new_test_dataset:\n",
    "    angle_list, true_class = data\n",
    "    predict_class = classifier.predict([angle_list])[0]\n",
    "    confusion_matrix[true_class, predict_class] +=1\n",
    "print(confusion_matrix)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(confusion_matrix)\n",
    "ax.set_title(\"confusion matrix\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"complex_confusion.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f8ce2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandGesture:\n",
    "    def __init__(self):\n",
    "        self.classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "        \n",
    "        with open(\"new_training_dataset.pkl\",\"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "        training_inputs = []\n",
    "        training_labels = []\n",
    "        for data in dataset:\n",
    "            angle_list, true_class = data\n",
    "            training_inputs.append(angle_list)\n",
    "            training_labels.append(true_class)\n",
    "        self.classifier.fit(training_inputs, training_labels)\n",
    "    \n",
    "    def get_prediction(self, hand_landmarks):\n",
    "        adjacency_list = [\n",
    "            [0,1,2],[1,2,3],[2,3,4],[0,5,6],[5,6,7],[6,7,8],\n",
    "            [0,9,10],[9,10,11],[10,11,12],[0,13,14],[13,14,15],\n",
    "            [14,15,16],[0,17,18],[17,18,19],[18,19,20]\n",
    "        ]\n",
    "\n",
    "        angle_list=[]\n",
    "\n",
    "        for adjacency in adjacency_list :\n",
    "            idx0, idx1, idx2 = adjacency\n",
    "            point0 = np.array([hand_landmarks.landmark[idx0].x, hand_landmarks.landmark[idx0].y, hand_landmarks.landmark[idx0].z])\n",
    "            point1 = np.array([hand_landmarks.landmark[idx1].x, hand_landmarks.landmark[idx1].y, hand_landmarks.landmark[idx1].z])\n",
    "            point2 = np.array([hand_landmarks.landmark[idx2].x, hand_landmarks.landmark[idx2].y, hand_landmarks.landmark[idx2].z])\n",
    "\n",
    "            vector0 = point0 - point1\n",
    "            vector1 = point2 - point1\n",
    "            inner_product_result = np.dot(vector0,vector1)\n",
    "            cos_theta = inner_product_result/(np.linalg.norm(vector0) * np.linalg.norm(vector1))\n",
    "            theta = np.arccos(cos_theta)\n",
    "            angle_list.append(theta)\n",
    "            \n",
    "        #predict_class = self.classifier.predict([angle_list])[0]\n",
    "        predict_probs = self.classifier.predict_proba([angle_list])[0]\n",
    "        \n",
    "        predict_class = np.argmax(predict_probs)\n",
    "        predict_prob = max(predict_probs)\n",
    "        \n",
    "        return predict_class, predict_prob\n",
    "    \n",
    "    def is_thumb_up(self, hand_landmarks):\n",
    "        thumb_mcp_idx = 2\n",
    "        thumb_tip_idx = 4\n",
    "        thumb_mcp_y = hand_landmarks.landmark[thumb_mcp_idx].y\n",
    "        thumb_tip_y = hand_landmarks.landmark[thumb_tip_idx].y\n",
    "        finger_up = thumb_tip_y < thumb_mcp_y\n",
    "        return finger_up\n",
    "    \n",
    "    def get_gesture(self, hand_landmarks):\n",
    "        prediction, predict_prob = self.get_prediction(hand_landmarks)\n",
    "        \n",
    "        if predict_prob < 0.7:\n",
    "            return \"none\"\n",
    "        elif prediction == 0:\n",
    "            return \"one\"\n",
    "        elif prediction == 1:\n",
    "            return \"two\"\n",
    "        elif prediction == 2:\n",
    "            return \"three\"\n",
    "        elif prediction == 3:\n",
    "            if self.is_thumb_up(hand_landmarks):\n",
    "                return \"thumb up\"\n",
    "            else:\n",
    "                return \"thumb down\"\n",
    "        elif prediction == 4:\n",
    "            return \"hand raise\"\n",
    "        \n",
    "    def show_gesture(self, gesture):\n",
    "        img_name = gesture.replace(' ', '_') + \".png\"\n",
    "        img = cv2.imread(f\"img/{img_name}\")\n",
    "        cv2.imshow(\"emoji\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d6f7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_gesture = HandGesture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fba1686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "hands = mp_hands.Hands(static_image_mode=False , max_num_hands = 1 , min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "\n",
    "history_len = 10\n",
    "gesture_history = [\"none\"]*history_len\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"read fail\")\n",
    "        continue\n",
    "        \n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.flags.writeable = False\n",
    "    results = hands.process(img)\n",
    "    img.flags.writeable = True\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    if results.multi_hand_landmarks is not None:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        mp_drawing.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        gesture = hand_gesture.get_gesture(hand_landmarks)\n",
    "    else:\n",
    "        gesture = \"none\"\n",
    "        \n",
    "    gesture_history.append(gesture)\n",
    "    gesture_history = gesture_history[1:]\n",
    "    max_gesture = \"none\"\n",
    "    max_cnt = 0\n",
    "    for gesture_name in [\"one\" , \"two\" , \"three\" , \"thumb up\" , \"thumb down\" , \"hand raise\" , \"none\"]:\n",
    "        cnt = gesture_history.count(gesture_name)\n",
    "        if max_cnt < cnt:\n",
    "            max_cnt = cnt\n",
    "            max_gesture = gesture_name\n",
    "    gesture = max_gesture\n",
    "    \n",
    "    cv2.putText(img, gesture , org=(50,50), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale = 1, color = (0,0,255))\n",
    "    hand_gesture.show_gesture(gesture)\n",
    "    \n",
    "    cv2.imshow(\"test\" , img)\n",
    "    if cv2.waitKey(5)==27:\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027f5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
